[
{
	"uri": "//localhost:1313/",
	"title": "Event-Driven Architecture with SNS, SQS, S3, API Gateway and Lambda",
	"tags": [],
	"description": "",
	"content": "Building an Event-Driven Architecture with AWS Overview In this lab, you\u0026rsquo;ll build a sophisticated event-driven architecture on AWS using SNS, SQS, and Lambda, and extend it with S3 triggers, API Gateway endpoints, and CloudWatch monitoring. You\u0026rsquo;ll design message patterns, configure dead-letter queues (DLQs), handle FIFO ordering, implement batch processing, and set up monitoring and cost optimization strategies. You\u0026rsquo;ll also explore operational procedures and build a basic testing framework for validation.\nThis lab guides you through the steps of architecting, implementing, and optimizing an asynchronous event-driven workflow that is scalable, fault-tolerant, and observable.\nArchitecture Goals Build an architecture with decoupled producers and consumers Enable message routing via SNS topics and SQS subscriptions Use standard queues and FIFO queues for appropriate message guarantees Configure dead-letter queues for error handling Apply batch processing for performance and cost efficiency Trigger workflows from S3 uploads and API Gateway POST calls Design monitoring dashboards and alerts using CloudWatch Implement cost-optimization techniques using AWS-native tools Document operational runbooks and test cases This workshop assumes you already have basic familiarity with AWS services like IAM, Lambda, and S3. You‚Äôll also benefit from knowledge of JSON message formats, retry policies, and async patterns.\nAmazon SNS (Simple Notification Service) SNS is a managed pub/sub messaging service that enables message fan-out to multiple subscribers. It helps route messages to multiple destinations like Lambda, SQS, and HTTP endpoints.\nAmazon SQS (Simple Queue Service) SQS is a fully managed message queuing service that enables decoupling of microservices. It supports standard queues (at-least-once delivery, best-effort ordering) and FIFO queues (exactly-once processing, strict ordering).\nAWS Lambda AWS Lambda is a serverless compute service that runs your backend code in response to triggers such as SQS messages, S3 events, and API Gateway HTTP calls. It scales automatically and supports concurrency settings and DLQ handling for robust message processing.\nAmazon S3 Amazon S3 is a storage service for objects like JSON files, images, and logs. You can trigger Lambda functions when a new file is uploaded to an S3 bucket, making it ideal for ingesting event data like user submissions or batch inputs.\nAmazon API Gateway API Gateway allows you to expose HTTPS endpoints to trigger AWS services. It integrates directly with Lambda to receive incoming requests (e.g., a quiz submission), which can then be routed via SNS or processed directly.\nAmazon CloudWatch CloudWatch is the monitoring, logging, and alerting backbone of AWS. You\u0026rsquo;ll use it to:\nView Lambda execution logs Monitor error rates and latency for SQS, Lambda, and API Gateway Set alarms and create dashboards for visibility Query logs using CloudWatch Logs Insights DLQ (Dead Letter Queue) A DLQ is a standard SQS queue used to capture failed messages after exceeding retry attempts. DLQs are critical for troubleshooting failures and maintaining resilience.\nMonitoring and Cost Optimization We\u0026rsquo;ll use CloudWatch Metrics, Logs, and Alarms to track system behavior. You\u0026rsquo;ll also use AWS Cost Explorer, CloudWatch Logs Insights, and Lambda cost calculators to optimize expenses.\nMain Content Setting up S3 and CRUD API Processing S3 Uploads via SQS-Triggered Lambda Routing Messages with SNS Topics and SQS Subscriptions Handling Failures Using Dead Letter Queues (DLQ) Ensuring Message Order with FIFO Queues Enabling Batch Processing in Lambda with SQS Setting up Monitoring and Logging with CloudWatch Optimizing for Cost and Performance in Event Flows Documenting Operational Procedures and Recovery Steps Building a Testing Framework for Event-Driven Workflows "
},
{
	"uri": "//localhost:1313/1-s3-and-crud-apis/",
	"title": "Setting up S3 and CRUD API",
	"tags": [],
	"description": "",
	"content": "Content:\nCreate S3 bucket Prepare sample quiz JSON Create Lambda functions Create IAM role and policy Attach IAM permissions Add Lambda code Configure API Gateway Attach IAM permissions Test CRUD operations Outcome Create S3 bucket Go to the S3 Console. Click Create bucket. Name it. Choose a region, for example: (ap-southeast-1). Leave the rest of the settings as default. Click Create bucket to finish setup. Prepare sample quiz JSON On your local machine, create a file named (quiz.json) with the following content, and upload that file to newly created bucket. { \u0026#34;id\u0026#34;: \u0026#34;quiz-001\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;AWS Basics Quiz\u0026#34;, \u0026#34;questions\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;q1\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Which AWS services are serverless?\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;multiple\u0026#34;, \u0026#34;options\u0026#34;: [\u0026#34;EC2\u0026#34;, \u0026#34;S3\u0026#34;, \u0026#34;Lambda\u0026#34;, \u0026#34;Fargate\u0026#34;], \u0026#34;answer\u0026#34;: [1, 2, 3] }, { \u0026#34;id\u0026#34;: \u0026#34;q2\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;What is the max size of an S3 object?\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;single\u0026#34;, \u0026#34;options\u0026#34;: [\u0026#34;5 GB\u0026#34;, \u0026#34;50 GB\u0026#34;, \u0026#34;5 TB\u0026#34;, \u0026#34;50 TB\u0026#34;], \u0026#34;answer\u0026#34;: [2] } ] } Create IAM role and policy Go to the IAM Console. Select Policies ‚Üí Create Policy. We will create 3 separate policies: Choose the JSON tab and paste the following: For s3ReadOnlyAccess: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::myprojectbucket1290/*\u0026#34; } ] } For s3PutOnlyAccess: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:PutObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::myprojectbucket1290/*\u0026#34; } ] } For s3RemoveOnlyAccess: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::myprojectbucket1290/*\u0026#34; } ] } Name the policy and create it.\nNow go to Roles ‚Üí Create Role. Each policy will have their own role\nSelect Lambda as the trusted entity type. Attach the policy you just created. Name the role and create it. Repeat the steps for s3PutOnlyAccess and s3RemoveOnlyAccess Final result: Create Lambda functions Create 4 Lambda functions using Node.js 20.x or 22.x. Use the following names and HTTP methods:\nAdd Lambda code Example: myGetFunction\nimport { S3Client, GetObjectCommand } from \u0026#34;@aws-sdk/client-s3\u0026#34;; import { Readable } from \u0026#34;stream\u0026#34;; const s3 = new S3Client({}); const BUCKET = \u0026#34;myprojectbucket1290\u0026#34;; //Change bucket name here const streamToString = (stream) =\u0026gt; new Promise((resolve, reject) =\u0026gt; { const chunks = []; stream.on(\u0026#34;data\u0026#34;, (chunk) =\u0026gt; chunks.push(chunk)); stream.on(\u0026#34;error\u0026#34;, reject); stream.on(\u0026#34;end\u0026#34;, () =\u0026gt; resolve(Buffer.concat(chunks).toString(\u0026#34;utf-8\u0026#34;))); }); export const handler = async (event) =\u0026gt; { const key = event.queryStringParameters?.key || \u0026#34;quiz.json\u0026#34;; console.log(\u0026#34;üîç GET request for key:\u0026#34;, key); try { const res = await s3.send(new GetObjectCommand({ Bucket: BUCKET, Key: key })); const data = await streamToString(res.Body); console.log(\u0026#34; Successfully retrieved:\u0026#34;, key); return { statusCode: 200, body: data, headers: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34; } }; } catch (err) { console.error(\u0026#34; Error reading object:\u0026#34;, err); return { statusCode: 500, body: JSON.stringify({ error: err.message }) }; } }; Repeat similarly for POST (create), PUT (update), and DELETE. Use the code below:\nExample: myPostFunction\nimport { S3Client, PutObjectCommand } from \u0026#34;@aws-sdk/client-s3\u0026#34;; const s3 = new S3Client({}); const BUCKET = \u0026#34;myprojectbucket1290\u0026#34;; export const handler = async (event) =\u0026gt; { const body = JSON.parse(event.body || \u0026#34;{}\u0026#34;); const key = body.key || \u0026#34;quiz.json\u0026#34;; const data = body.data; console.log(\u0026#34; POST request:\u0026#34;, { key, data }); if (!data) { console.warn(\u0026#34; Missing \u0026#39;data\u0026#39; in request\u0026#34;); return { statusCode: 400, body: \u0026#34;Missing \u0026#39;data\u0026#39; in request body\u0026#34; }; } try { await s3.send(new PutObjectCommand({ Bucket: BUCKET, Key: key, Body: JSON.stringify(data), ContentType: \u0026#34;application/json\u0026#34; })); console.log(\u0026#34; Created quiz:\u0026#34;, key); return { statusCode: 200, body: `Created quiz: ${key}` }; } catch (err) { console.error(\u0026#34; Error creating object:\u0026#34;, err); return { statusCode: 500, body: JSON.stringify({ error: err.message }) }; } }; Example: myPutFunction\nimport { S3Client, PutObjectCommand } from \u0026#34;@aws-sdk/client-s3\u0026#34;; const s3 = new S3Client({}); const BUCKET = \u0026#34;myprojectbucket1290\u0026#34;; export const handler = async (event) =\u0026gt; { const key = event.queryStringParameters?.key || \u0026#34;quiz.json\u0026#34;; const body = JSON.parse(event.body || \u0026#34;{}\u0026#34;); const data = body.data; console.log(\u0026#34; PUT request to update:\u0026#34;, { key, data }); if (!data) { console.warn(\u0026#34; Missing \u0026#39;data\u0026#39; in request\u0026#34;); return { statusCode: 400, body: \u0026#34;Missing \u0026#39;data\u0026#39; in request body\u0026#34; }; } try { await s3.send(new PutObjectCommand({ Bucket: BUCKET, Key: key, Body: JSON.stringify(data), ContentType: \u0026#34;application/json\u0026#34; })); console.log(\u0026#34; Updated quiz:\u0026#34;, key); return { statusCode: 200, body: `Updated quiz: ${key}` }; } catch (err) { console.error(\u0026#34;‚ùå Error updating object:\u0026#34;, err); return { statusCode: 500, body: JSON.stringify({ error: err.message }) }; } }; Example: myDeleteFunction\nimport { S3Client, DeleteObjectCommand } from \u0026#34;@aws-sdk/client-s3\u0026#34;; const s3 = new S3Client({}); const BUCKET = \u0026#34;myprojectbucket1290\u0026#34;; export const handler = async (event) =\u0026gt; { const key = event.queryStringParameters?.key || \u0026#34;quiz.json\u0026#34;; console.log(\u0026#34; DELETE request for:\u0026#34;, key); try { await s3.send(new DeleteObjectCommand({ Bucket: BUCKET, Key: key })); console.log(\u0026#34; Deleted quiz:\u0026#34;, key); return { statusCode: 200, body: `Deleted quiz: ${key}` }; } catch (err) { console.error(\u0026#34; Error deleting object:\u0026#34;, err); return { statusCode: 500, body: JSON.stringify({ error: err.message }) }; } }; Testing Test all functions to confirm they‚Äôre working properly.\nmyGetFunction myPostFunction\nfor Post and Put functions, use this as request body: { \u0026#34;key\u0026#34;: \u0026#34;quiz-aws.json\u0026#34;, \u0026#34;data\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;quiz-aws\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;AWS Quiz\u0026#34;, \u0026#34;questions\u0026#34;: [] } } myPutFunction myDeleteFunction\nFor Delete function, use this in body: { \u0026#34;queryStringParameters\u0026#34;: { \u0026#34;key\u0026#34;: \u0026#34;quiz-aws.json\u0026#34; } } Configure API Gateway Go to the API Gateway Console. Create a new HTTP API, name it QuizAPI. Add created Lambda functions as intergration Add the following routes and map them to their corresponding Lambda functions: Deploy to a default stage with auto-deploy on Final result:\n###5. Test with Postman\nFrom API Gateway dashboard, choose your api gateway Next, click on your api gateway name to view the detail Copy the invoke URL and open postman for testing: Outcome You now have a complete CRUD-capable quiz API backed by:\n(S3) for JSON file storage (Lambda) for serverless execution logic (API Gateway) for HTTP interface (JSON) as the quiz file format This setup forms the foundation of your serverless, event-driven learning platform.\n"
},
{
	"uri": "//localhost:1313/2-processing-s3-upload-events-with-sqs-and-lambda/",
	"title": " Processing S3 uploads via SQS-triggered Lambda",
	"tags": [],
	"description": "",
	"content": "Content:\nOverview Create SQS queue Allow S3 to send messages to SQS Add S3 Event Notification Create Lambda function for SQS Attach Lambda trigger Test the flow Overview In this step, you\u0026rsquo;ll implement a serverless event-driven flow:\n(S3 ‚Üí SQS ‚Üí Lambda)\nWhenever a new .json quiz file is uploaded to your S3 bucket, it triggers an SQS message. That message then invokes a Lambda function to process the uploaded file.\nDefinitions Amazon S3 (Simple Storage Service): Object storage service for storing any amount of data. It supports event notifications when new objects are created.\nAmazon SQS (Simple Queue Service): A fully managed message queuing service that enables you to decouple and scale microservices.\nLambda Trigger: An AWS mechanism that invokes your function in response to events (e.g., from SQS or S3).\nS3 Event Notification: A feature that lets S3 notify other services like Lambda, SNS, or SQS when certain events (e.g., file upload) occur in a bucket.\nCreate SQS queue Go to the Amazon SQS console. Click Create queue. Choose Standard Queue ‚Äî allows high throughput and at-least-once delivery. Name the queue: (QuizUploadQueue). Leave default settings and click Create Queue. Allow S3 to send messages to SQS To let S3 send messages to your queue, attach a queue access policy.\nOpen (QuizUploadQueue). Go to Access policy ‚Üí click Edit. Replace the policy with: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Statement1\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;sqs:SendMessage\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:sqs:ap-southeast-1:466992855491:myprojectqueue\u0026#34; //Replace this with your SQS queue ARN } ] } This policy allows S3 to push messages only from your specific bucket.\nAdd S3 Event Notification Configure S3 to send a message to the SQS queue whenever a .json file is created.\nGo to S3 ‚Üí (your-SQS-queue). Select the Properties tab. Scroll to Event notifications ‚Üí Click Create event notification. Configure the following: Save the configuration. üîé This creates a direct connection from S3 to SQS, only for .json files uploaded to this bucket.\nCreate IAM Policy and Role for Lambda Before creating the Lambda function, you need to give it permission to:\nRead files from the S3 bucket\nReceive messages from the SQS queue\nWrite logs to CloudWatch\nCreate IAM Policy\nGo to the IAM Console ‚Üí Policies ‚Üí Create policy\nChoose JSON and paste the following permissions:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;sqs:DeleteMessage\u0026#34;, \u0026#34;sqs:SendMessage\u0026#34;, \u0026#34;sqs:GetQueueAttributes\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:sqs:ap-southeast-1:466992855491:myprojectqueue\u0026#34; } ] } Click Next, give it a name like: QuizLambdaSQSAccessPolicy\nClick Create policy\nCreate IAM Role for Lambda Go to IAM Console ‚Üí Roles ‚Üí Create role\nTrusted entity: Select (Lambda)\nPermissions: Attach the policy you just created\nName the role: QuizLambdaExecutionRole\nClick Create role\nCreate Lambda function for SQS Create a Lambda function to process files triggered by SQS messages.\nGo to AWS Lambda ‚Üí Create function. Name: (QuizProcessFromSQS) Runtime: (Node.js 20.x or 22.x) Permissions: Attach the new Role you just created Paste the following code and deploy:\nimport { S3Client, GetObjectCommand } from \u0026#34;@aws-sdk/client-s3\u0026#34;; const s3 = new S3Client({}); const BUCKET = \u0026#34;myprojectbucket1290\u0026#34;; export const handler = async (event) =\u0026gt; { console.log(\u0026#34;SQS Event:\u0026#34;, JSON.stringify(event)); for (const record of event.Records) { const body = JSON.parse(record.body); const s3Info = body.Records?.[0]?.s3; if (!s3Info) continue; const key = decodeURIComponent(s3Info.object.key.replace(/\\+/g, \u0026#34; \u0026#34;)); console.log(\u0026#34;New File Uploaded:\u0026#34;, key); const res = await s3.send(new GetObjectCommand({ Bucket: BUCKET, Key: key })); const data = await res.Body.transformToString(); console.log(\u0026#34;File content:\u0026#34;, data); } return { statusCode: 200 }; }; Attach Lambda trigger Go to SQS ‚Üí QuizUploadQueue. Select Lambda triggers tab. Add: (QuizProcessFromSQS) as the consumer function. This means whenever SQS gets a new message, it will automatically trigger this Lambda.\nTest the flow Upload a new file to S3 (e.g., (quiz-new.json)) via the console. Observe these expected results: ‚úÖ Checkpoint Description SQS receives message Message from S3 triggered Lambda is triggered Logs in CloudWatch confirm it ran File name is logged S3 key is printed to the logs File content is printed Lambda reads and displays file text "
},
{
	"uri": "//localhost:1313/3-routing-messages-with-sns-topics-and-sqs-subscriptions/",
	"title": "Routing Messages with SNS Topics and SQS Subscriptions",
	"tags": [],
	"description": "",
	"content": "Content:\nIntroduction Architecture Overview Step-by-Step Implementation Create SNS Topic Create SQS Queues Subscribe SQS to SNS Create Lambda Consumer Create Lambda to Publish to SNS Create API Gateway Endpoint Expected Behavior Goal\nAllow your users or frontend clients to send quiz-related events (such as creating a new quiz) by calling an HTTP endpoint. This request goes through API Gateway, is processed by a Lambda function, published to SNS, and distributed (fan-out) to one or more SQS queues. Each queue then triggers its own Lambda consumer to process the message independently.\nArchitecture Overview Pattern: (API Gateway ‚Üí Lambda ‚Üí SNS ‚Üí SQS ‚Üí Lambda)\nInvolved Services:\nAPI Gateway ‚Äì Receives external HTTP requests Lambda (Submit) ‚Äì Publishes messages to SNS SNS Topic ‚Äì Broadcasts the message to SQS queues SQS Queues ‚Äì Store events for consumers Lambda (Consumer) ‚Äì Reads from SQS and handles processing Why This Step? This step introduces asynchronous event-based processing into your architecture. It allows you to:\nDecouple frontend/API clients from backend logic Process events in parallel across multiple consumers Add new consumers in the future without changing the API Improve fault-tolerance and scalability Instead of sending data directly to S3 or a Lambda, clients simply call your API Gateway. Everything behind the scenes is handled via events.\nStep-by-Step Implementation 1. Create SNS Topic Go to SNS ‚Üí Topics ‚Üí Create topic Select Standard Name it (QuizEventTopic) Click Create topic Copy the Topic ARN for later use 2. Subscribe SQS to SNS Go to SNS ‚Üí QuizEventTopic ‚Üí Create subscription Protocol: Amazon SQS Endpoint: Paste SQS Queue ARN Confirm the subscription 3. Create Lambda Consumer (SQS ‚Üí Lambda) Go to AWS Lambda ‚Üí Create function Name: ProcessQuizEvent Runtime: Node.js or Go Go to Function Permission -\u0026gt; Edit -\u0026gt; Add created role that contain: Add Trigger: SQS ‚Üí QuizProcessingQueue Paste this sample handler in the code section:\nexport const handler = async (event) =\u0026gt; { console.log(\u0026#34;SQS Event Received:\u0026#34;, JSON.stringify(event)); return { statusCode: 200 }; }; Go to IAM -\u0026gt; Role and add new policy (AWSLambdaBasicExecutionRole) for logging: 5. Create new SNS Role Go to IAM -\u0026gt; Policy -\u0026gt; Create new policy Paste this JSON as follow: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;sns:Publish\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Create a new role and attach the policy you just created 6. Create Lambda to Publish to SNS (API Gateway Backend) Go to AWS Lambda ‚Üí Create function Name: SubmitQuizEvent Runtime: Node.js Attach the new role you just created to this function Paste this sample code:\nimport { SNSClient, PublishCommand } from \u0026#34;@aws-sdk/client-sns\u0026#34;; const sns = new SNSClient({}); const TOPIC_ARN = \u0026#34;arn:aws:sns:ap-southeast-1:YOUR_ACCOUNT_ID:QuizEventTopic\u0026#34;; //Copy and Paste your SNS Topic ARN here export const handler = async (event) =\u0026gt; { const body = JSON.parse(event.body || \u0026#34;{}\u0026#34;); const message = { default: JSON.stringify(body), }; try { await sns.send( new PublishCommand({ TopicArn: TOPIC_ARN, Message: JSON.stringify(message), MessageStructure: \u0026#34;json\u0026#34;, }) ); return { statusCode: 200, body: JSON.stringify({ message: \u0026#34;Quiz event submitted.\u0026#34; }), }; } catch (err) { console.error(\u0026#34; Publish failed\u0026#34;, err); return { statusCode: 500, body: JSON.stringify({ error: err.message }), }; } }; 6. Create API Gateway Endpoint Go to API Gateway ‚Üí Create API Choose: HTTP API Set Integration: SubmitQuizEvent Lambda Create route: POST /quiz-event Deploy the API Test using (curl) or (Postman): curl -X POST https://your-api-id.amazonaws.com/quiz-event \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;quiz_id\u0026#34;: \u0026#34;quiz001\u0026#34;, \u0026#34;event\u0026#34;: \u0026#34;created\u0026#34; }\u0026#39; Result: Checking using Cloud Watch: Expected Behavior The client sends a POST request to API Gateway API Gateway triggers the (SubmitQuizEvent) Lambda The Lambda publishes to the SNS topic SNS fans out the message to one or more SQS queues Each SQS queue triggers a Lambda consumer Logs confirm message flow and processing "
},
{
	"uri": "//localhost:1313/4-handling-failures-using-dead-letter-queues-dlq/",
	"title": "Handling Failures Using Dead Letter Queues (DLQ)",
	"tags": [],
	"description": "",
	"content": "Content:\nGoal Updated Architecture Step-by-Step Implementation Create DLQ Attach DLQ to Main Queue Simulate Consumer Failure Send Test Message Verify DLQ Fix and Restore Summary Goal Improve fault tolerance by capturing failed messages that cannot be processed after multiple retries ‚Äî instead of silently dropping them.\nüß± What is a DLQ (Dead-Letter Queue)? A DLQ (Dead-Letter Queue) is a separate (SQS queue) used to store messages that fail multiple times during processing. When a consumer (such as a Lambda function) tries to process a message and fails repeatedly (based on the (MaxReceiveCount) setting), that message is automatically redirected to the DLQ for later inspection.\nThis design prevents your main queue and processing logic from being blocked or overwhelmed by problematic messages, and provides a safety net in event-driven systems.\n‚úÖ Why Use a DLQ? DLQs are essential for building resilient and fault-tolerant architectures. They allow you to:\nPrevent data loss\nMessages that can\u0026rsquo;t be processed won‚Äôt be deleted or lost ‚Äî instead, they\u0026rsquo;re redirected to the DLQ safely.\nInvestigate message failures\nYou can inspect failed payloads and metadata in the DLQ to find out why processing failed (e.g., malformed JSON, missing fields, downstream service errors).\nReplay messages after fixing issues\nAfter fixing the root cause (e.g., updating code or data structure), you can manually move the message back to the main queue for reprocessing.\nStep-by-Step 1. Create a DLQ (Dead-Letter Queue) Go to SQS ‚Üí Create queue Select Type: Standard Name: QuizDLQ Leave other settings as default Click Create queue 2. Attach DLQ to Your Main Queue Open your main SQS queue\nClick Edit\nScroll to the Dead-letter queue section\nConfigure:\n(DLQ ARN): Choose (QuizDLQ) (MaxReceiveCount): Set to (3) (This means after 3 failed Lambda retries, the message is sent to DLQ)\nClick (Save changes) 3. Simulate a Lambda Consumer Failure Temporarily update your (ProcessQuizEvent) Lambda to fail:\nReplace your handler code with this:\nexport const handler = async (event) =\u0026gt; { console.log(\u0026#34; Simulated Failure:\u0026#34;, JSON.stringify(event)); throw new Error(\u0026#34;Simulated processing failure\u0026#34;); }; This will force every incoming message to fail.\n4. Send a Test Message Use your existing flow:\nSend a POST request to (POST /quiz-event) via (API Gateway) The message will flow through: (API Gateway ‚Üí Lambda ‚Üí SNS ‚Üí SQS ‚Üí Lambda (fails 3 times) ‚Üí DLQ)\n5. Check the DLQ Go to (SQS ‚Üí QuizDLQ) Click (Send and receive messages) Click (Poll for messages) Click on a message ‚Üí View full details You‚Äôll see the failed event stored here Restore the Original Lambda Once testing is complete:\nRevert your (ProcessQuizEvent) Lambda to its correct logic Save and redeploy Your architecture is now fault-tolerant and can safely route failing messages to a DLQ for investigation.\nSummary (DLQ) is a backup queue for failed messages No data is lost ‚Äî messages can be inspected or replayed Useful for debugging, error tracking, and retrying failed events Combined with retries, this is a core fault-tolerance mechanism in AWS event-driven systems "
},
{
	"uri": "//localhost:1313/5-ensuring-message-order-with-fifo-queues/",
	"title": "Ensuring Message Order with FIFO Queues",
	"tags": [],
	"description": "",
	"content": " Goal Ensure that:\nQuiz events are processed in the exact order they occur (e.g., (created ‚Üí updated ‚Üí deleted)) Duplicate messages are automatically prevented What is a (FIFO Queue)? FIFO stands for First-In, First-Out\nA FIFO queue guarantees:\nOrdered Delivery: Messages are processed in the order they were sent (within the same (MessageGroupId)) Exactly-Once Processing: No duplicate messages are delivered (if your consumer is idempotent) Why Should You Use FIFO Queues? In distributed systems, events may arrive (out of order) or get retried. Without ordering guarantees, a later update (e.g., (quiz deletion)) may be processed before an earlier one (e.g., (quiz creation)), leading to incorrect state or data corruption.\nFIFO queues solve this by:\nMaintaining strict sequence of operations Avoiding double-processing, especially important in financial or scoring events Improving data integrity, which is critical in audit logs, version history, and transactional pipelines When to Use (FIFO Queues) Use FIFO queues when: Processing order matters (e.g., score updates, versioned content) You must avoid duplicated processing You need strong consistency Step-by-Step: Implementing FIFO-Based Flow 1. Create a (FIFO Queue) Go to (Amazon SQS) ‚Üí (Create queue) Choose (FIFO queue) Name it: (QuizFifoQueue.fifo) Enable: (Content-based deduplication) ‚Äî or specify your own (MessageDeduplicationId) Click (Create) 2. Subscribe (FIFO Queue) to an Event Source Because SNS Standard does not support direct delivery to FIFO queues.\nSo we modify your Lambda (SubmitQuizEvent) to send directly to the FIFO SQS queue instead of SNS. 3. Update the (Publisher Lambda) to Send to (FIFO SQS) Modify your (SubmitQuizEvent) Lambda function:\nimport { SQSClient, SendMessageCommand } from \u0026#34;@aws-sdk/client-sqs\u0026#34;; const sqs = new SQSClient({}); const QUEUE_URL = \u0026#34;https://sqs.ap-southeast-1.amazonaws.com/YOUR_ID/QuizFifoQueue.fifo\u0026#34;; export const handler = async (event) =\u0026gt; { const body = JSON.parse(event.body || \u0026#34;{}\u0026#34;); const params = { QueueUrl: QUEUE_URL, MessageBody: JSON.stringify(body), MessageGroupId: \u0026#34;quiz-events\u0026#34;, // Required // Optional: MessageDeduplicationId // MessageDeduplicationId: body.id || Date.now().toString(), }; try { await sqs.send(new SendMessageCommand(params)); return { statusCode: 200, body: JSON.stringify({ message: \u0026#34;Event sent to FIFO queue.\u0026#34; }), }; } catch (err) { console.error(\u0026#34;Send failed\u0026#34;, err); return { statusCode: 500, body: JSON.stringify({ error: err.message }), }; } }; Create the Consumer Lambda\nGo to AWS Lambda ‚Üí Create function\nName it: ProcessQuizFifoEvent\nAttach role with sqs access\nRuntime: Node.js 20.x or Node.js 22.x\nTrigger: QuizFifoQueue.fifo\nPaste the code below:\nexport const handler = async (event) =\u0026gt; { console.log(\u0026#34; FIFO Event:\u0026#34;, JSON.stringify(event)); return { statusCode: 200 }; }; Expected Behavior You send a (POST /quiz-event) via (API Gateway) The (SubmitQuizEvent) Lambda sends the payload to (QuizFifoQueue.fifo) (FIFO) ensures (message ordering) and (deduplication) The (ProcessQuizFifoEvent) Lambda processes events (in order) "
},
{
	"uri": "//localhost:1313/6-enabling-batch-processing-in-lambda-with-sqs/",
	"title": "Enabling Batch Processing in Lambda with SQS",
	"tags": [],
	"description": "",
	"content": "Overview This section outlines the recommended best practices for implementing batch message processing using AWS Lambda with Amazon SQS. This design pattern is a key component in scalable, cost-effective, and resilient event-driven architectures.\nWhat is Batch Processing? Batch processing refers to the execution of a group of tasks or messages together within a single unit of work.\nIn AWS, this means one Lambda function invocation handles multiple SQS messages at once rather than processing them individually.\nThis approach enhances performance and reduces the number of invocations, which in turn reduces cost and latency.\nArchitectural Rationale Batch processing is a recommended practice in serverless environments for the following reasons:\nOperational Efficiency\nMinimizes Lambda invocations by consolidating multiple messages into a single execution cycle.\nThroughput Optimization\nReduces idle time and cold start frequency, enabling faster overall processing and lower latency under burst loads.\nCost Optimization\nReduces the number of billable Lambda invocations, directly impacting overall execution costs.\nScalability and Resilience\nHandles high-frequency workloads gracefully. In conjunction with features like partial batch response and DLQs, batch processing enhances fault isolation and retry reliability.\nDesign Flexibility\nProvides better control over message ordering, timeouts, and concurrency management using FIFO queues and batch windowing.\nImplementation Guide 1. Configure Lambda Trigger from SQS Navigate to AWS Lambda \u0026gt; your target function.\nUnder the Triggers section, locate and edit the SQS trigger.\nModify the following parameters:\nBatch size Up to 10 for FIFO queues Up to 10,000 for Standard queues Batch window (optional) Duration (0‚Äì300 seconds): Lambda waits to buffer more messages before invoking. 2. Update Lambda Handler to Process Batches The Lambda function must loop over event.Records to process each message individually within the batch:\nexport const handler = async (event) =\u0026gt; { console.log(` Received batch of ${event.Records.length} messages`); for (const record of event.Records) { const body = JSON.parse(record.body); console.log(\u0026#34; Processing message:\u0026#34;, body); // Business logic here } return { statusCode: 200 }; }; 3. Test the Batch Setup Send multiple messages to the SQS queue in quick succession. Confirm batch aggregation by reviewing CloudWatch Logs. Expected Output: Failure Simulation and DLQ Integration To test failure handling:\nForce an error in one message‚Äôs processing loop. Observe that the entire batch is retried by default. Note: Enable Partial Batch Response to allow successful messages to complete while failing messages are retried.\nConfigure a Dead Letter Queue (DLQ) for the SQS queue to capture failed messages after maxReceiveCount is reached.\nMonitoring and Observability Use the following CloudWatch Metrics for insight and alerting:\nApproximateNumberOfMessagesNotVisible ‚Üí Messages currently being processed NumberOfMessagesDeleted ‚Üí Successfully processed and removed from the queue NumberOfMessagesReceived ‚Üí Incoming workload IteratorAge ‚Üí Delay between message enqueue and processing Enable structured logging and consider using AWS X-Ray for tracing.\nSummary Batch processing in Lambda provides a scalable and cost-efficient way to process high-throughput SQS workloads. With proper tuning and observability, it becomes a foundational pattern in resilient, event-driven cloud architectures.\n"
},
{
	"uri": "//localhost:1313/7-setting-up-monitoring-and-logging-with-cloudwatch/",
	"title": "Monitoring and Observability with Amazon CloudWatch",
	"tags": [],
	"description": "",
	"content": "What is Amazon CloudWatch? Amazon CloudWatch is a fully managed observability service that provides data and actionable insights for AWS resources, applications, and services. It enables you to:\nCollect logs and metrics from AWS services and custom applications Create alarms based on thresholds or anomaly detection Visualize data with dashboards Respond automatically to changes using events and rules CloudWatch plays a central role in diagnosing operational issues and optimizing performance in a distributed, event-driven, or serverless architecture.\nWhy Use Logging and CloudWatch? Establishing proper logging and observability with CloudWatch is essential for the following reasons:\nTroubleshooting and Debugging:\nLogs help you trace issues down to the exact function or event, allowing you to resolve errors faster.\nOperational Monitoring:\nMetrics allow you to track system health ‚Äî including function invocation rates, latency, and error frequency.\nFailure Detection:\nAlarms notify you proactively of problems like message backlog in queues, throttling, or DLQ overflow.\nAudit and Compliance:\nLogs serve as a historical record of system behavior for audit trails and post-incident reviews.\nPerformance Optimization:\nCloudWatch metrics can be used to analyze bottlenecks, optimize compute/memory usage, and reduce cost.\nObjective Implement centralized monitoring and observability for your serverless application using Amazon CloudWatch. This includes:\nCapturing logs from Lambda, API Gateway, SQS, and SNS Gathering operational metrics such as invocations, errors, and latency Setting up alarms to detect anomalies or failures Creating dashboards to visualize system performance and health Architecture Components Under Observation The following AWS components should be integrated with CloudWatch:\nComponent Metrics and Logs to Monitor AWS Lambda Invocations, duration, errors, throttles Amazon SQS Messages sent, visible messages, DLQ queue length Amazon SNS Message publish success/failure Amazon API Gateway Request count, 4xx/5xx errors, latency Amazon S3 Optional: access logs, request metrics Implementation Steps 1. Enable Logging for Lambda Functions Lambda automatically integrates with CloudWatch Logs if the execution role includes the appropriate permissions.\nRequired IAM Policy Ensure your Lambda function‚Äôs IAM role includes the following permissions:\n{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } Verification:\nNavigate to the Lambda function Go to Configuration ‚Üí Permissions Confirm the IAM role includes AWSLambdaBasicExecutionRole or equivalent If not: Go to IAM -\u0026gt; Role -\u0026gt; Choose the role used in Lambda Add permissions -\u0026gt; Attach policies Choose AWSLambdaBasicExecutionRole then Add permission 2. Create a New Log Group in CloudWatch Navigate to Amazon CloudWatch via the AWS Management Console.\nIn the left-hand menu, select (Logs) ‚Üí Log groups, then click Create log group.\nEnter a name for the log group, set the retention policy to 1 day, and choose the Log class as Standard.\nClick Create, then copy the Amazon Resource Name ‚Äî ARN of the new log group. 2. Enable Logging in API Gateway For HTTP APIs:\nOpen API Gateway in the AWS Management Console Select your API and then the desired Stage On the left sidebar, choose Monitor -\u0026gt; Logging: Enable Access Logging Set Log Level to INFO or ERROR Specify a CloudWatch Log Group (create one if necessary) Recommended Log Format $context.requestId $context.httpMethod $context.resourcePath $context.status 3. View and Analyze Logs in CloudWatch Navigate to CloudWatch ‚Üí Log groups Locate the log group corresponding to your Lambda or API Gateway (e.g., (/aws/lambda/my-function)) Open logs to inspect request/response flow, exceptions, or debug outputs Optional: Use CloudWatch Logs Insights CloudWatch Logs Insights provides a query interface to search logs. Example query:\nfields @timestamp, @message | filter @message like /error/ | sort @timestamp desc 4. Configure Alarms for Critical Metrics Set up alarms in (CloudWatch ‚Üí Alarms) to detect operational issues such as:\nMetric Description Lambda (Errors) Triggers when function errors \u0026gt; 0 DLQ message count Detects if messages accumulate in DLQ API Gateway (5xx errors) Identifies upstream/internal service issues SQS message backlog Use (ApproximateNumberOfMessagesVisible) Alarms should be tied to actionable alerts (e.g., via SNS topic or email).\n5. Build a Centralized CloudWatch Dashboard Create a custom dashboard in (CloudWatch ‚Üí Dashboards) with the following widgets:\nWidget Title Metrics Displayed Lambda Performance Invocations, Errors, Duration API Gateway Error Rates 4xx and 5xx error counts and latency SQS Queue Depth Message count in queues DLQ Monitoring DLQ queue size over time Use these visualizations for a real-time overview of system health and performance.\nExpected Results After implementing the above:\nAll application components continuously stream logs to CloudWatch Alarms detect and alert on system errors and delays Dashboards enable real-time insights into performance and reliability Logs and metrics allow for efficient debugging, root cause analysis, and operations monitoring "
},
{
	"uri": "//localhost:1313/8-optimizing-for-cost-and-performance-in-event-flows/",
	"title": "Optimizing for Cost and Performance in Event Flows",
	"tags": [],
	"description": "",
	"content": "Goal Apply cost-saving techniques and performance optimizations across your serverless architecture without sacrificing reliability. This ensures your design is efficient, scalable, and affordable for production workloads and long-term operation.\nWhy Should We Optimize? In event-driven, serverless systems, resources like Lambda, SQS, and CloudWatch scale automatically ‚Äî which is convenient, but also increases the risk of silent cost escalation.\nOptimizing ensures:\nYou only pay for what you need Your architecture remains performant under load You can scale confidently without unplanned billing spikes Logging and monitoring do not generate excessive overhead Cost Benefit Proper optimization results in:\nArea Benefit Lambda Tuning Lower execution time = fewer billing ms SQS Batching Fewer invocations = reduced costs Log Retention Policies Lower CloudWatch storage cost S3 Intelligent Tiering Reduced cost for infrequent access data Fewer Unused Resources Avoid background charges entirely Optimization Targets Component Cost Factors Optimization Techniques Lambda Duration, memory, invocations Right-size memory, reduce cold starts, reuse shared code API Gateway Request volume Use HTTP API (cheaper than REST API) SQS Number of requests Batch processing, long polling SNS Requests, fan-out Avoid unnecessary fan-out to unused endpoints CloudWatch Logs, metrics Apply log retention policy, consolidate dashboards S3 Storage, PUT requests Use compression, intelligent tiering, limit write ops Step-by-Step Optimization Plan 1. Optimize Lambda Costs Navigate to (Lambda ‚Üí Configuration ‚Üí General Configuration)\nAdjust Memory: Increasing memory may reduce execution time Identify the optimal configuration using real-time testing Use provisioned concurrency only when necessary Avoid unless strict latency guarantees are needed Reduce Cold Starts: Prefer (HTTP API) over (REST API) for better integration reuse Minimize redundant code: Factor out shared libraries across functions 2. Use HTTP API HTTP APIs are approximately 70% cheaper than REST APIs Simpler, lower latency, and ideal for Lambda integration 3. Enable Batch Processing in SQS Configure your Lambda consumer to process messages in batches: Recommended: (Batch size = 5‚Äì10) Benefits: Reduces number of Lambda invocations Decreases overall compute cost Enable (long polling) in SQS to reduce cost from empty receives 4. Minimize CloudWatch Log Costs Navigate to (CloudWatch ‚Üí Log Groups)\nSet log retention policy (e.g., 7 or 14 days) Remove unused log groups after testing phase Consolidate logs into dashboards for visibility without duplication 5. S3 Storage and PUT Optimization Use compressed JSON format for large payloads Enable (S3 Intelligent-Tiering) for cost-effective long-term storage Avoid frequent PUT operations (each write adds cost) 6. Delete Unused Resources After each test iteration:\nDelete any unused or temporary: Lambda functions SQS queues and SNS topics API Gateway stages or endpoints This prevents idle resources from incurring charges over time.\n7. Monitor Cost with AWS Cost Explorer Navigate to (AWS Cost Explorer)\nEnable service if not already done Create cost reports segmented by: Day Service (Lambda, SQS, API Gateway) Linked usage types This helps identify costly services before they scale unexpectedly.\nExpected Outcome Serverless cost becomes predictable and controlled Logs are retained only as long as needed Unused resources are proactively cleaned up Lambda functions operate at optimized performance/memory settings Real-time monitoring supports proactive budgeting and alerts "
},
{
	"uri": "//localhost:1313/9-documenting-operational-procedures-and-recovery-steps/",
	"title": "Exposing External Event Submission via API Gateway",
	"tags": [],
	"description": "",
	"content": "Testing and Validation Strategy for Event-Driven Architectures Goal Establish a comprehensive testing strategy to ensure your event-driven architecture is:\nFunctionally correct: Each component behaves according to its specification Resilient to failure: Systems recover gracefully from runtime issues or network faults Scalable under load: Message throughput and Lambda execution scale with demand Consistent with guarantees: Ordering, deduplication, and retry behaviors are preserved Testing such systems involves simulating real-world data flows, validating message delivery chains, observing failure responses, and confirming monitoring visibility.\nWhat is an Event-Driven Architecture? An event-driven architecture (EDA) is built around the generation, propagation, and handling of discrete messages (events) across loosely coupled services. It typically involves:\nProducers that emit events (e.g., API Gateway, S3, user actions) Messaging middleware such as SNS and SQS to route or buffer events Consumers like Lambda functions that respond to those events Ensuring that these asynchronous flows work correctly under real-world conditions requires testing at multiple levels.\nTesting Layers Divide your testing into three key levels to isolate failures, validate behavior, and guarantee end-to-end reliability.\n1. Unit Testing (Lambda Logic) Focus: Isolated validation of each Lambda function‚Äôs core logic.\nMock input events to simulate API Gateway, SQS, or SNS payloads Validate return values, output formats, error handling, and conditional paths Does not require actual AWS infrastructure Recommended Tools:\nNode.js: Jest, Vitest, Mocha Go: built-in testing package (testing) AWS SDK mocks: (aws-sdk-client-mock) for JS, (gomock) for Go Example (Node.js):\n( // submitQuizEvent.test.js\nimport { handler } from \u0026ldquo;./submitQuizEvent.js\u0026rdquo;;\ntest(\u0026ldquo;submits a message\u0026rdquo;, async () =\u0026gt; { const result = await handler({ body: JSON.stringify({ quiz_id: \u0026ldquo;q123\u0026rdquo; }) }); expect(result.statusCode).toBe(200); }); )\n2. Integration Testing (Service-to-Service) Focus: Validate communication between real AWS services such as Lambda, SQS, SNS, and S3.\nDeploy services using CDK, SAM, or manually Test message routing, retry policies, queue behavior, and permissions Confirm Lambda triggers work from S3 uploads, SNS fan-out, or SQS batching Scenarios to Cover:\nSNS ‚Üí SQS ‚Üí Lambda: Fan-out pattern S3 file upload triggers Lambda FIFO queue enforces strict order (MessageGroupId) DLQ catches messages after failed retries How to Perform:\nUse AWS Console or CLI to send messages or upload files Use CloudWatch Logs to trace Lambda behavior Simulate failures by throwing errors in Lambda code and observe DLQ behavior 3. End-to-End Testing (Full Message Flow) Focus: Simulate real user behavior and validate the entire event chain from request to processing.\nExample Scenario:\nUser sends POST request to API Gateway Event is published to SNS Fan-out sends message to multiple SQS queues Each SQS queue triggers a consumer Lambda Consumers process events, or send failed ones to DLQ Sample Test (curl):\n( curl -X POST https://your-api-id.amazonaws.com/quiz-event -H \u0026ldquo;Content-Type: application/json\u0026rdquo; -d \u0026lsquo;{ \u0026ldquo;quiz_id\u0026rdquo;: \u0026ldquo;q001\u0026rdquo;, \u0026ldquo;event\u0026rdquo;: \u0026ldquo;created\u0026rdquo; }\u0026rsquo; )\nWhat to Test Aspect Test Example Functional Valid messages are parsed and processed correctly Error Handling Lambda throws ‚Üí message ends up in DLQ Ordering FIFO queue preserves message order Batching Lambda receives batch size of N from SQS Monitoring Logs, metrics, alarms reflect system state Replay Messages from DLQ can be manually replayed Tools You Can Use Tool Purpose AWS Console Manual message injection and Lambda test runs AWS CLI / SDK Scripted testing and automation CloudWatch Logs Review logs, error stacks, execution traces CloudWatch Insights Structured queries across log streams Postman / curl Simulate API Gateway calls Unit test runners Isolate and validate core Lambda logic Optional: Automate Your Tests Automation is useful for CI/CD pipelines, or scheduled validation in production-like environments.\nStrategies:\nUse GitHub Actions or AWS CodePipeline to deploy and run tests Use Python or Bash scripts to send synthetic events to SNS/SQS Schedule periodic Lambda tests using CloudWatch EventBridge Rules Automatically check DLQ queue length and publish alerts Success Criteria A well-tested event-driven system should meet the following:\nAll Lambda functions return valid responses under expected load No unprocessed messages in DLQs under normal conditions FIFO queues maintain correct order with deduplication working Monitoring confirms event paths with visibility in CloudWatch Logs and Metrics Edge cases and failure scenarios are testable and recoverable Summary Testing event-driven systems involves multiple testing layers‚Äîfrom unit-level Lambda logic to end-to-end flows through SNS, SQS, and API Gateway. These tests ensure reliability, scalability, and operational clarity, especially when combined with logging, metrics, and alarms.\nConsistently validating your architecture under both normal and adverse conditions ensures that your application can withstand traffic spikes, input anomalies, and partial outages‚Äîall while providing a predictable and maintainable developer experience.\n"
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]